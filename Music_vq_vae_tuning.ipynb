{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oghj-VpeperO"
      },
      "source": [
        "# Before you start\n",
        "Open this link and create a shortcut to indices_genres in your drive: https://drive.google.com/file/d/1-0CjAdc5ZJIw_pxu8ycVBmHnJabAIxEg/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjBkx6Gxpjsh"
      },
      "source": [
        "# Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIkvQetyvaVU",
        "outputId": "a64e3006-e1bf-4a4f-fb9b-f1a7a903dc32"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    colab = True\n",
        "except:\n",
        "    colab = False\n",
        "print (\"Running colab:\", colab)\n",
        "path = \"/content/\" if colab else \"\"\n",
        "#abs path\n",
        "import os\n",
        "path = os.path.abspath(path) + \"/\"\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klCWMwu0VAPR",
        "outputId": "bb582a29-75b5-4840-8e4c-ae8ed07287b2"
      },
      "outputs": [],
      "source": [
        "#if not os.path.exists(path + \"mel_specs_music\"):\n",
        "!mkdir mel_specs_music\n",
        "!mkdir mel_specs_music/train\n",
        "!mkdir mel_specs_music/val\n",
        "!mkdir mel_specs_music/train/cl\n",
        "!mkdir mel_specs_music/val/cl\n",
        "!git clone https://github.com/nadavbh12/VQ-VAE.git\n",
        "!mv ./VQ-VAE/ ./VQ_VAE/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFtfCZiZjLD4",
        "outputId": "a752743a-cc12-47d0-ccc4-a5b8b361c0fb"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from VQ_VAE.vq_vae.auto_encoder import VQ_CVAE\n",
        "import tqdm\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import cv2\n",
        "import scipy\n",
        "from gc import collect\n",
        "import IPython.display as ipd\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtYE9uvTy_lY",
        "outputId": "e3f6c1f7-2159-4868-8175-56323f4c6f9e"
      },
      "outputs": [],
      "source": [
        "#@title Unzip\n",
        "# Make sure your processed mel spectrogram data is in the same directory as this notebook\n",
        "if colab: \n",
        "  !unzip '/content/drive/MyDrive/Big Data Project/large_music_mel_spectrograms.zip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class vq_vae_search:\n",
        "    def __init__(self, bs=64, dict_size=128, epochs=15, h=64, w=256):\n",
        "        self.bs = bs\n",
        "        self.dict_size = dict_size\n",
        "        self.epochs = epochs\n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.path = path\n",
        "        self.vqvae_music_checkpoints_folder = path + 'vqvae_music_checkpoints/'\n",
        "        self.filter_and_org_data()\n",
        "        self.tune_aug_hyperperameters()\n",
        "        self.train()\n",
        "        #self.load_model()\n",
        "        #self.predict_values() \n",
        "        #self.top_k(50)\n",
        "  \n",
        "    def filter_and_org_data(self):\n",
        "        if not os.path.exists(self.vqvae_music_checkpoints_folder):\n",
        "            os.mkdir(self.vqvae_music_checkpoints_folder)\n",
        "        self.ds_dir = path + 'mel_specs_music/val/'\n",
        "        self.img = glob.glob(path + 'music_mel_spectrograms/*.png')\n",
        "        if colab:\n",
        "            with open('/content/drive/MyDrive/indices_genres', 'rb') as f:\n",
        "                self.ig = pickle.load(f)\n",
        "        else:\n",
        "            with open('indices_genres', 'rb') as f:\n",
        "                self.ig = pickle.load(f)\n",
        "        #filter, undersample, and reorganize data\n",
        "        self.img_ids = list(map(lambda x: os.path.basename(x)[:-4], self.img))\n",
        "        #print(self.img_ids)\n",
        "        self.df_filter = pd.DataFrame.from_dict({'genre': [self.ig[el] for el in self.img_ids], 'id':self.img}).dropna()\n",
        "        self.df_filter = pd.concat([self.df_filter[self.df_filter['genre'] == genre][:1000].reset_index(drop=True) for genre in list(self.df_filter['genre'].value_counts()[:-7].keys())]).reset_index(drop=True)\n",
        "        self.img = list(self.df_filter['id'].values)\n",
        "\n",
        "        self.img_train, self.img_test = train_test_split(self.img, test_size=0.2, random_state=42)\n",
        "        self.f_train = lambda x: path + 'mel_specs_music/train/cl/' + os.path.basename(x)\n",
        "        self.f_test = lambda x: path + 'mel_specs_music/val/cl/' + os.path.basename(x)\n",
        "        self.out_train = list(map(self.f_train, self.img_train))\n",
        "        self.out_test = list(map(self.f_test, self.img_test))\n",
        "\n",
        "        for el1, el2 in zip(self.img_train, self.out_train):\n",
        "            #shutil.move(el1, el2)\n",
        "            shutil.copy(el1, el2)\n",
        "\n",
        "        for el1, el2 in zip(self.img_test, self.out_test):\n",
        "            #shutil.move(el1, el2)\n",
        "            shutil.copy(el1, el2)\n",
        "\n",
        "    def tune_aug_hyperperameters(self):\n",
        "        #@title Tune augmentation hyperparameters\n",
        "        self.size = f'transforms.Resize(({self.h}, {self.w}))'\n",
        "        self.replace_main_with = '''dataset_transforms = {\n",
        "            'custom': transforms.Compose([transforms.Grayscale(), transforms.Resize((h, w)), \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.5), (0.5))]),\n",
        "            'imagenet': transforms.Compose([transforms.Grayscale(), transforms.Resize((h, w)), \n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.5), (0.5))]),\n",
        "            'cifar10': transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5), (0.5))]),\n",
        "            'mnist': transforms.ToTensor()\n",
        "        }'''.replace('transforms.Resize((h, w))', self.size)\n",
        "        self.replace_main = '''dataset_transforms = {\n",
        "            'custom': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "            'imagenet': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256),\n",
        "                                            transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "            'cifar10': transforms.Compose([transforms.ToTensor(),\n",
        "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n",
        "            'mnist': transforms.ToTensor()\n",
        "        }'''\n",
        "\n",
        "        with open(path + 'VQ_VAE/main.py', 'r') as f:\n",
        "            self.data = f.read()\n",
        "\n",
        "        self.data = self.data.replace(self.replace_main, self.replace_main_with)\n",
        "\n",
        "        with open(path + 'VQ_VAE/main.py', 'w') as f:\n",
        "            f.write(self.data)\n",
        "\n",
        "\n",
        "        self.replace_main_with = '''dataset_n_channels = {\n",
        "            'custom': 1,\n",
        "            'imagenet': 1,\n",
        "            'cifar10': 1,\n",
        "            'mnist': 1,\n",
        "        }'''\n",
        "        self.replace_main = '''dataset_n_channels = {\n",
        "            'custom': 3,\n",
        "            'imagenet': 3,\n",
        "            'cifar10': 3,\n",
        "            'mnist': 1,\n",
        "        }'''\n",
        "\n",
        "        with open(path + 'VQ_VAE/main.py', 'r') as f:\n",
        "            self.data = f.read()\n",
        "\n",
        "        self.data = self.data.replace(self.replace_main, self.replace_main_with)\n",
        "\n",
        "        with open(path + 'VQ_VAE/main.py', 'w') as f:\n",
        "            f.write(self.data)\n",
        "\n",
        "    def train(self):\n",
        "        #@title Train and save to drive (be sure to save the previous trained model somewhere else as the checkpoint folder will be emptied)\n",
        "        VQVAE_path = path + 'VQ_VAE'\n",
        "        %cd $VQVAE_path\n",
        "        if colab:\n",
        "            !python3 main.py --dataset=custom --model=vqvae --data-dir=/content/mel_specs_music --epochs={self.epochs} --batch-size {self.bs} --dict-size {self.dict_size}\n",
        "            self.checkpoint_path = sorted(glob.glob('/content/VQ_VAE/results/*/checkpoints/*.pth'))[-1]\n",
        "        else:\n",
        "            !python3 main.py --dataset=custom --model=vqvae --data-dir=../mel_specs_music --epochs={self.epochs} --batch-size {self.bs} --dict-size {self.dict_size}\n",
        "            self.checkpoint_path = sorted(glob.glob('../VQ_VAE/results/*/checkpoints/*.pth'))[-1]\n",
        "\n",
        "        !rm -rf {self.vqvae_music_checkpoints_folder}\n",
        "        !mkdir {self.vqvae_music_checkpoints_folder}\n",
        "\n",
        "        shutil.copyfile(self.checkpoint_path, os.path.join(self.vqvae_music_checkpoints_folder, os.path.basename(self.checkpoint_path)))\n",
        "\n",
        "    def load_model(self, channels=1):\n",
        "        #@title Load model\n",
        "        self.model = VQ_CVAE(128, k = self.dict_size, num_channels=channels)\n",
        "        self.model.load_state_dict(torch.load(self.checkpoint_path))\n",
        "\n",
        "    def predict_values(self):\n",
        "        #@title Predict values\n",
        "        if not colab:\n",
        "            !cd ~./VQ-VAE-SEARCH\n",
        "        self.T = transforms.Compose([transforms.Grayscale(), transforms.Resize((self.h, self.w)), #transforms.CenterCrop(256),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5), (0.5))])\n",
        "\n",
        "        self.test_dataset = torch.utils.data.DataLoader(datasets.ImageFolder(self.ds_dir, transform=self.T), batch_size=self.bs, shuffle=False)\n",
        "\n",
        "        self.categories = []\n",
        "        self.data_names = datasets.ImageFolder(self.ds_dir, transform=self.T)\n",
        "        self.data_names = [el[0] for el in self.data_names.samples]\n",
        "        self.test_ids = list(map(lambda x: os.path.basename(x)[:-4], self.data_names))\n",
        "        self.categories = [self.ig[el] for el in self.test_ids]\n",
        "\n",
        "        self.normalize = lambda x: (x - x.min()) / (x.max() - x.min())\n",
        "\n",
        "        self.all_outputs = []\n",
        "        i = 0\n",
        "        for data, _ in tqdm.tqdm(self.test_dataset):\n",
        "            with torch.no_grad():\n",
        "                self.outputs = self.model(data)[2]\n",
        "                self.outputs = self.outputs.reshape(self.outputs.shape[0], -1).detach().cpu().numpy() #outputs[1] = enc, outputs[2] = emb\n",
        "            self.all_outputs.append(self.outputs)\n",
        "            i += 1\n",
        "\n",
        "        self.all_outputs = np.concatenate(self.all_outputs, 0)\n",
        "        #self.all_outputs = self.normalize(all_outputs)\n",
        "\n",
        "        with open(f'{self.vqvae_music_checkpoints_folder}/all_outputs.pickle', 'wb') as f:\n",
        "            pickle.dump(self.all_outputs, f)\n",
        "\n",
        "    def top_k(self, k=50):\n",
        "        #@title Top K data\n",
        "        self.n_comp=k\n",
        "        with open(f'{self.vqvae_music_checkpoints_folder}/all_outputs.pickle', 'rb') as f:\n",
        "            self.all_outputs = pickle.load(f)\n",
        "\n",
        "        self.categories = []\n",
        "        self.data_names = datasets.ImageFolder(self.ds_dir, transform=self.T)\n",
        "        self.data_names = [el[0] for el in self.data_names.samples]\n",
        "        self.test_ids = list(map(lambda x: os.path.basename(x)[:-4], self.data_names))\n",
        "        self.categories = [self.ig[el] for el in self.test_ids]\n",
        "\n",
        "        self.spec_paths = datasets.ImageFolder(self.ds_dir, transform=self.T)\n",
        "        self.spec_paths = [el[0] for el in self.spec_paths.imgs]\n",
        "\n",
        "        self.pca = PCA(n_components=self.n_comp)\n",
        "        self.pca_result = self.pca.fit_transform(self.all_outputs)\n",
        "        self.pca_df = pd.DataFrame(self.pca_result, columns=list(map(str,list(range(0, self.n_comp)))))\n",
        "        self.pca_df['categories'] = self.categories\n",
        "        self.pca_df['spec_paths'] = self.spec_paths\n",
        "\n",
        "    def show_top_k(self, select_id=599, k=10):\n",
        "        #@title Top K\n",
        "        self.vecs = self.pca_df[list(map(str,list(range(0, self.n_comp))))].to_numpy()\n",
        "        self.distances = sklearn.metrics.pairwise.cosine_similarity(self.vecs, self.vecs)\n",
        "        self.top_k = np.flip(np.argsort(self.distances[select_id]))[:k]\n",
        "        self.top_k_df = self.pca_df.iloc[self.top_k][['categories', 'spec_paths']]\n",
        "        self.audios = [self.png_to_audio(spec) for spec in self.top_k_df['spec_paths'].values]\n",
        "        display(self.top_k_df)\n",
        "\n",
        "    def display_audio(self, audios):\n",
        "        for audio in audios:\n",
        "            display(audio)\n",
        "\n",
        "    def pca(self):\n",
        "        self.pca = PCA(n_components=3)\n",
        "        self.pca_result = self.pca.fit_transform(self.all_outputs)\n",
        "        self.pca_df = pd.DataFrame(self.pca_result, columns=['1', '2', '3'])\n",
        "        self.pca_df['categories'] = self.categories\n",
        "\n",
        "        fig = px.scatter_3d(self.pca_df, x='1', y='2', z='3', color='categories', width=1500)\n",
        "        fig.show()\n",
        "\n",
        "    def tsne(self):\n",
        "        #@title TSNE (requires tuning)\n",
        "        self.pca_tsne = PCA(n_components=50) # change the number of components as a hyperparameter\n",
        "        self.pca_tsne_result = self.pca_tsne.fit_transform(self.all_outputs)\n",
        "\n",
        "        self.tsne = TSNE(n_components=3, verbose=1, perplexity=25, n_iter=3000, learning_rate=200)\n",
        "        self.tsne_results = self.tsne.fit_transform(self.pca_tsne_result)\n",
        "        self.tsne_df = pd.DataFrame(self.tsne_results, columns=['1', '2', '3'])\n",
        "        self.tsne_df['categories'] = self.categories\n",
        "\n",
        "        fig = px.scatter_3d(self.tsne_df, x='1', y='2', z='3', color='categories', width=1200)\n",
        "        fig.show()\n",
        "\n",
        "    def png_to_audio(self, audio_file='spec.png', n_fft = 2000, hop_length = 150, win = 50, mi = -80.0, m = 0.0, sr = 22050, save=False):\n",
        "        # read image\n",
        "        spec = cv2.imread(audio_file, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # de_normalize\n",
        "        spec = (spec * (m - mi) / 255) + mi\n",
        "        spec = spec.astype(np.float32)\n",
        "\n",
        "        # from spectrogram to audio\n",
        "        aud = self.from_spectrogram(spec, n_fft=n_fft, hop_length=hop_length, win=win, sr=sr)\n",
        "\n",
        "        # save audio\n",
        "        aud = self.play_audio(aud)\n",
        "        if save:\n",
        "            with open(f'{audio_file[0:-4]}.wav', 'wb') as f:\n",
        "                    f.write(aud.data)\n",
        "        return aud\n",
        "\n",
        "    def play_audio(self, audio_file, sr=22050):\n",
        "        if type(audio_file) == str:\n",
        "            return ipd.Audio(audio_file,  rate=sr)\n",
        "        else:\n",
        "            return ipd.Audio(audio_file, rate=sr)\n",
        "\n",
        "    def from_spectrogram(self, spectrogram,  hop_length=150, n_fft=2000, win=50, sr = 22050):\n",
        "        # undo power_to_db\n",
        "        S = spectrogram\n",
        "        S = librosa.db_to_power(S)\n",
        "        S = librosa.feature.inverse.mel_to_audio(S, sr=sr, n_fft=n_fft, hop_length=hop_length, window=win)\n",
        "        return S\n",
        "\n",
        "    \n",
        "        "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test dict size: 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "id": "5LXgjzgtiv03",
        "outputId": "7d7b14f0-8262-4f96-d199-22516361a101"
      },
      "outputs": [],
      "source": [
        "model_dict_32 = vq_vae_search(bs=64, dict_size=32, epochs=15, h=64, w=256)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_dict_32.load_model(1)\n",
        "model_dict_32.predict_values() \n",
        "model_dict_32.top_k(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_32.show_top_k(select_id=599, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_32.pca()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_32.tsne()\n",
        "\n",
        "del model_dict_32\n",
        "collect()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test dict size: 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_64 = vq_vae_search(bs=64, dict_size=64, epochs=15, h=64, w=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_64.show_top_k(select_id=599, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_64.pca()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_64.tsne()\n",
        "del model_dict_64\n",
        "collect()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test dict size: 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_128 = vq_vae_search(bs=64, dict_size=128, epochs=15, h=64, w=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_128.show_top_k(select_id=599, k=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_128.pca()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_dict_128.tsne()\n",
        "del model_dict_128\n",
        "collect()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Oghj-VpeperO"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "18f39d5a9bfe4d0ce9b1ccd808a3754df6677d81d118bc81d2886eb8b9b7056c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
