{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zWfrHtpz0pbf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import itertools\n",
        "import gc\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Omv8PhrnmPmG"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(torch.nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        \n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "        \n",
        "        self._embedding = torch.nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "        \n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "        \n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "            \n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "        \n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "        \n",
        "        # Loss\n",
        "        e_latent_loss = torch.nn.functional.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = torch.nn.functional.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "        \n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "        \n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PQsosddb1NyX"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(self, n_input_channels, hidden_size, latent_dim):\n",
        "    super().__init__()\n",
        "    self.model = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(n_input_channels, hidden_size, kernel_size=3, stride=2, padding=1),\n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.Conv2d(hidden_size, hidden_size, kernel_size=3),\n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.Conv2d(hidden_size, 2*hidden_size, kernel_size=3),\n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.Conv2d(2*hidden_size, 2*hidden_size, kernel_size=3, stride=2),\n",
        "        torch.nn.GELU(),\n",
        "        #torch.nn.Flatten()\n",
        "    )\n",
        "\n",
        "    #self.linear_mean = torch.nn.Linear(2*hidden_size*25, latent_dim)\n",
        "    #self.linear_logvar = torch.nn.Linear(2*hidden_size*25, latent_dim)\n",
        "    #self.linear = torch.nn.Linear(2*hidden_size*16, latent_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    #x = self.linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tEJJPsXK6UbO"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(self, n_input_channels, hidden_size, latent_dim):\n",
        "    super().__init__()\n",
        "    #self.linear = torch.nn.Sequential(torch.nn.Linear(latent_dim, 2 * 16 * hidden_size), torch.nn.GELU())\n",
        "\n",
        "    self.model = torch.nn.Sequential(\n",
        "        torch.nn.ConvTranspose2d(2*hidden_size, 2*hidden_size, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.Conv2d(2*hidden_size, 2*hidden_size, kernel_size=3, padding=1),\n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.ConvTranspose2d(2*hidden_size, hidden_size, kernel_size=3, stride=2, output_padding=1, padding=1), \n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.Conv2d(hidden_size, hidden_size, kernel_size=3), # , padding=1\n",
        "        torch.nn.GELU(),\n",
        "        torch.nn.ConvTranspose2d(hidden_size, n_input_channels, kernel_size=3, stride=2, output_padding=1, padding=1),\n",
        "        torch.nn.Tanh(),\n",
        "        #torch.nn.Sigmoid(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x = self.linear(x)\n",
        "    #x = x.reshape(x.shape[0], -1, 4, 4)\n",
        "    x = self.model(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DiaPnvn9A_jZ"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(torch.nn.Module):\n",
        "  def __init__(self, n_input_channels, hidden_size, latent_dim, num_embeddings, embedding_dim, commitment_cost):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(n_input_channels, hidden_size, latent_dim)\n",
        "    self.decoder = Decoder(n_input_channels, hidden_size, latent_dim)\n",
        "    self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    loss, quantized, perplexity, enc = self._vq_vae(x)\n",
        "    x = self.decoder(quantized)\n",
        "    return x, enc, loss, perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A4Xxbj7UGFQC"
      },
      "outputs": [],
      "source": [
        "def visualize_grid(x_batch):\n",
        "  im_rec = Image.fromarray(torchvision.utils.make_grid((x_batch*0.5+0.5) * 255).permute(1, 2, 0).cpu().numpy().astype(np.uint8))\n",
        "  #im_rec = Image.fromarray(torchvision.utils.make_grid((x_batch) * 255).permute(1, 2, 0).cpu().numpy().astype(np.uint8))\n",
        "  return im_rec.resize((im_rec.size[0]*4, im_rec.size[1]*4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# None-tuned perameters\n",
        "batch_size = 128\n",
        "shuffle_train = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "    torchvision.transforms.Resize((28, 28)),\n",
        "    transforms.ToTensor(),\n",
        "     #torchvision.transforms.ConvertImageDtype(dtype=torch.uint8),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     ])\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "    torchvision.transforms.Resize((28, 28)),\n",
        "    #transforms.RandomHorizontalFlip(0.2),\n",
        "    #transforms.RandomRotation(10),\n",
        "    #transforms.RandomEqualize(.5),\n",
        "    transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5), (0.5)),\n",
        "     ])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=shuffle_train, num_workers=8)\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if type(trainset.data) is np.ndarray:\n",
        "    data_variance = np.var(trainset.data / 255.0)\n",
        "else:\n",
        "    data_variance = torch.var(trainset.data / 255.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SkU_V8b2BpRB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"n_input_channels, hidden_size, latent_dim = 1, 28, 24\n",
        "num_embeddings, embedding_dim, commitment_cost = 128, 56, 0.25 # embedding_dim = num_channels at the output of the encoder\n",
        "learning_rate = 1e-3\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = Autoencoder(n_input_channels, hidden_size, latent_dim, num_embeddings, embedding_dim, commitment_cost).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\"\"\"\n",
        "if trainset.data.shape[-1] == 3: \n",
        "    n_input_channels, hidden_size, latent_dim = 3, 28, 24\n",
        "else:\n",
        "    n_input_channels, hidden_size, latent_dim = 1, 28, 24\n",
        "    \n",
        "num_embeddings, embedding_dim, commitment_cost = 128, 56, 0.25 # embedding_dim = num_channels at the output of the encoder\n",
        "learning_rate = 1e-3\n",
        "\n",
        "trials = {}\n",
        "for num_embeddings in range(5, 350, 5): \n",
        "    for embedding_dem in range(5, 350, 5): \n",
        "        trials[num_embeddings,embedding_dim] = []\n",
        "\n",
        "\n",
        "# for num_embeddings,embedding_dim in trials.keys():\n",
        "#     print(num_embeddings,embedding_dim)\n",
        "#     device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "#     model = Autoencoder(n_input_channels, hidden_size, latent_dim, num_embeddings, embedding_dim, commitment_cost).to(device)\n",
        "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "#     for epoch in range(50):  # loop over the dataset multiple times\n",
        "#         running_loss = 0.0\n",
        "#         for i, data in enumerate(trainloader, 0):\n",
        "#             # get the inputs; data is a list of [inputs, labels]\n",
        "#             inputs, labels = data\n",
        "#             inputs = inputs.to(device)\n",
        "#             # zero the parameter gradients\n",
        "#             optimizer.zero_grad()\n",
        "#             # forward + backward + optimize\n",
        "#             outputs, encodings, loss, perplexity = model(inputs)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             # print statistics\n",
        "#             running_loss += loss.item()\n",
        "#             if i % 100 == 99:    # print every 100 mini-batches\n",
        "#                 print('[%d, %5d] loss: %.3f' %\n",
        "#                       (epoch + 1, i + 1, running_loss / 100))\n",
        "#                 running_loss = 0.0\n",
        "#         trials[num_embeddings,embedding_dim].append(loss.item())\n",
        "#         print(\"Epoch\", epoch, \"loss\", loss.item())\n",
        "\n",
        "#     print('Finished Training')\n",
        "\n",
        "#     for i, data in enumerate(testloader, 0):\n",
        "#         inputs, labels = data\n",
        "#         inputs = inputs.to(device)\n",
        "#         outputs, encodings, loss, perplexity = model(inputs)\n",
        "#         im_rec = visualize_grid(outputs)\n",
        "#         im_rec.save(\"test.png\")\n",
        "#         break\n",
        "\n",
        "#     print(\"Perplexity\", perplexity.item())\n",
        "\n",
        "#     for i, data in enumerate(trainloader, 0):\n",
        "#         inputs, labels = data\n",
        "#         inputs = inputs.to(device)\n",
        "#         outputs, encodings, loss, perplexity = model(inputs)\n",
        "#         im_rec = visualize_grid(outputs)\n",
        "#         im_rec.save(\"train.png\")\n",
        "#         break\n",
        "\n",
        "#     print(\"Perplexity\", perplexity.item())\n",
        "\n",
        "#     torch.save(model.state_dict(), \"model.pt\")\n",
        "\n",
        "#     #with open(\"model.pt\", \"wb\") as f:\n",
        "#     #    torch.save(model, f)\n",
        "\n",
        "#     #with open(\"model.pt\", \"rb\") as f:\n",
        "#     #    model = torch.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "shape '[-1, 5]' is invalid for input of size 114688",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [11], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m x_hat, enc, vq_loss, perplexity \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     15\u001b[0m mse_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mmse_loss(x, x_hat) \u001b[39m/\u001b[39m data_variance\n\u001b[1;32m     16\u001b[0m \u001b[39m#mse_loss = mse_loss.sum(dim=[1, 2, 3]).mean(dim=[0])\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#bce_loss = torch.nn.functional.binary_cross_entropy(x_hat.view(-1, 1024), x.view(-1, 1024), reduction='sum')\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#loss = bce_loss + kl_loss \u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn [5], line 11\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     10\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[0;32m---> 11\u001b[0m   loss, quantized, perplexity, enc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_vq_vae(x)\n\u001b[1;32m     12\u001b[0m   x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(quantized)\n\u001b[1;32m     13\u001b[0m   \u001b[39mreturn\u001b[39;00m x, enc, loss, perplexity\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "Cell \u001b[0;32mIn [2], line 18\u001b[0m, in \u001b[0;36mVectorQuantizer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     15\u001b[0m input_shape \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mshape\n\u001b[1;32m     17\u001b[0m \u001b[39m# Flatten input\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m flat_input \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_embedding_dim)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Calculate distances\u001b[39;00m\n\u001b[1;32m     21\u001b[0m distances \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39msum(flat_input\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n\u001b[1;32m     22\u001b[0m             \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msum(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding\u001b[39m.\u001b[39mweight\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m             \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mmatmul(flat_input, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mt()))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 5]' is invalid for input of size 114688"
          ]
        }
      ],
      "source": [
        "\n",
        "for num_embeddings,embedding_dim in trials.keys():\n",
        "    learning_rate = 1e-3\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    model = Autoencoder(n_input_channels, hidden_size, latent_dim, num_embeddings, embedding_dim, commitment_cost).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "    for epoch in range(7):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            x, labels = data\n",
        "            x = x.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_hat, enc, vq_loss, perplexity = model(x)\n",
        "            mse_loss = torch.nn.functional.mse_loss(x, x_hat) / data_variance\n",
        "            #mse_loss = mse_loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
        "            #bce_loss = torch.nn.functional.binary_cross_entropy(x_hat.view(-1, 1024), x.view(-1, 1024), reduction='sum')\n",
        "            #loss = bce_loss + kl_loss \n",
        "            loss = mse_loss + vq_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % batch_size * 5 == batch_size * 5 - 5:\n",
        "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / (batch_size * 5):.3f}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "          for g in optimizer.param_groups:\n",
        "            learning_rate *= 0.1\n",
        "            g['lr'] = learning_rate\n",
        "    trials[num_embeddings,embedding_dim].append(running_loss / (batch_size * 5))\n",
        "    gc.collect()\n",
        "with open('tune_hyperperameters.pkl', 'bw') as f:\n",
        "        pickle.dump(trials, f)\n",
        "print(trials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('tune_hyperperameters.txt', 'w') as f:\n",
        "    for key, value in trials.items():\n",
        "        f.write(f'num_embeddings:{key[0]} - embedding_dim:{key[1]} - loss:{value}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trials.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxU0kJxyFemS"
      },
      "outputs": [],
      "source": [
        "for data in testloader:\n",
        "  x, labels = data\n",
        "  x = x.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    x_hat = model(x)[0]\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# with open('tune_hyperperameters.pkl', 'br') as f:\n",
        "#     trials = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bpO7JxA1mDyd",
        "outputId": "49ba56e6-bc66-478f-fe2e-6bceb144875d"
      },
      "outputs": [],
      "source": [
        "visualize_grid(x_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "suxcg6W0H3ul",
        "outputId": "c5d5df74-762b-4c81-9fb6-bc274123c233"
      },
      "outputs": [],
      "source": [
        "visualize_grid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk2CiYbhtU_e"
      },
      "outputs": [],
      "source": [
        "means, lbls = [], []\n",
        "for data in testloader:\n",
        "  x, labels = data\n",
        "  x = x.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    x_mean = model.encoder(x)[0]\n",
        "  means.append(x_mean)\n",
        "  lbls.append(labels)\n",
        "\n",
        "features = torch.cat(means,0)\n",
        "features = features.detach().cpu().numpy()\n",
        "\n",
        "nsamples, nx, ny = features.shape\n",
        "features = features.reshape((nsamples,nx*ny))\n",
        "\n",
        "labels = torch.cat(lbls).numpy()\n",
        "\n",
        "print(features.shape, labels.shape)\n",
        "\n",
        "tsne = TSNE(n_components=2,learning_rate='auto',init='pca',perplexity=30).fit_transform(features) #,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyRPkjPyxrLX"
      },
      "outputs": [],
      "source": [
        "colors = np.array([\"red\",\"green\",\"blue\",\"yellow\",\"pink\",\"black\",\"orange\",\"purple\",\"beige\",\"brown\"])\n",
        "c = np.array([colors[el] for el in labels])\n",
        "tsne_sel = tsne#[(labels==1)|(labels==4)]\n",
        "#col_sel = c#[(labels==3)|(labels==5)]\n",
        "#plt.scatter(tsne_sel[:,0], tsne_sel[:,1], c=col_sel)\n",
        "plt.scatter(tsne_sel[:,0], tsne_sel[:,1]) #,c=col_sel)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "0a25effbdb37c891a2e9d4084264472a304386d1b4f4a3dd9d5e57807f471f70"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
